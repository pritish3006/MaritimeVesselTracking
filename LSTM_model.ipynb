{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECT_ID</th>\n",
       "      <th>VID</th>\n",
       "      <th>SEQUENCE_DTTM</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>SPEED_OVER_GROUND</th>\n",
       "      <th>COURSE_OVER_GROUND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100008</td>\n",
       "      <td>14:00:00</td>\n",
       "      <td>36.906850</td>\n",
       "      <td>-76.089022</td>\n",
       "      <td>1</td>\n",
       "      <td>1641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>100015</td>\n",
       "      <td>14:00:00</td>\n",
       "      <td>36.950000</td>\n",
       "      <td>-76.026834</td>\n",
       "      <td>11</td>\n",
       "      <td>2815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>100016</td>\n",
       "      <td>14:00:00</td>\n",
       "      <td>36.906783</td>\n",
       "      <td>-76.089084</td>\n",
       "      <td>0</td>\n",
       "      <td>2632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>100019</td>\n",
       "      <td>14:00:00</td>\n",
       "      <td>37.003000</td>\n",
       "      <td>-76.283167</td>\n",
       "      <td>148</td>\n",
       "      <td>2460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>100016</td>\n",
       "      <td>14:00:01</td>\n",
       "      <td>36.906783</td>\n",
       "      <td>-76.089084</td>\n",
       "      <td>0</td>\n",
       "      <td>2632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECT_ID     VID SEQUENCE_DTTM        LAT        LON  SPEED_OVER_GROUND  \\\n",
       "0          1  100008      14:00:00  36.906850 -76.089022                  1   \n",
       "1          2  100015      14:00:00  36.950000 -76.026834                 11   \n",
       "2          3  100016      14:00:00  36.906783 -76.089084                  0   \n",
       "3          4  100019      14:00:00  37.003000 -76.283167                148   \n",
       "4          5  100016      14:00:01  36.906783 -76.089084                  0   \n",
       "\n",
       "   COURSE_OVER_GROUND  \n",
       "0                1641  \n",
       "1                2815  \n",
       "2                2632  \n",
       "3                2460  \n",
       "4                2632  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "ais_data = pd.read_csv('set1.csv')\n",
    "\n",
    "ais_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 20 unique ships accounted for in the dataset. Now, we will first split the dataset into test, validate and train sets. Since the total number of datapoints in this dataset is 13714, we will split the data on the basis of the time value - `SEQUENCE_DTTM` and then use the train-validate-test sets to train the prediction models, validate it and then test it. We will use the first 80% of the data for training, the next 15% for validation and the last 5% for testing.\n",
    "\n",
    "Here, we will use the following three prediction models on this dataset:\n",
    "    \n",
    "      1. LSTM\n",
    "      2. LSTNet\n",
    "      3. Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the `OBJECT_ID` column\n",
    "ais_data = ais_data.drop('OBJECT_ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17:59:58'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returning the last time value present in the `SEQUENCE_DTTM` column\n",
    "ais_data[\"SEQUENCE_DTTM\"].iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we split the 'SEQUENCE_DTTM' column into `HOUR` and `TIME` columns. This will help us in the future when we convert the dataset into a tensor and feed it into the LSTM model. We will also drop the `SEQUENCE_DTTM` column as it is no longer required. Further, this conversion would allow us to create a tensor that is of the shape (NxCxL) where N is the number of samples, C is the number of channels and L is the length of the sequence. This is the format that the LSTM model requires.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HOUR</th>\n",
       "      <th>TIME</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>SPEED_OVER_GROUND</th>\n",
       "      <th>COURSE_OVER_GROUND</th>\n",
       "      <th>VID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.906850</td>\n",
       "      <td>-76.089022</td>\n",
       "      <td>1</td>\n",
       "      <td>1641</td>\n",
       "      <td>100008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.950000</td>\n",
       "      <td>-76.026834</td>\n",
       "      <td>11</td>\n",
       "      <td>2815</td>\n",
       "      <td>100015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.906783</td>\n",
       "      <td>-76.089084</td>\n",
       "      <td>0</td>\n",
       "      <td>2632</td>\n",
       "      <td>100016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.003000</td>\n",
       "      <td>-76.283167</td>\n",
       "      <td>148</td>\n",
       "      <td>2460</td>\n",
       "      <td>100019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36.906783</td>\n",
       "      <td>-76.089084</td>\n",
       "      <td>0</td>\n",
       "      <td>2632</td>\n",
       "      <td>100016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  HOUR  TIME        LAT        LON  SPEED_OVER_GROUND  COURSE_OVER_GROUND  \\\n",
       "0    0     0  36.906850 -76.089022                  1                1641   \n",
       "1    0     0  36.950000 -76.026834                 11                2815   \n",
       "2    0     0  36.906783 -76.089084                  0                2632   \n",
       "3    0     0  37.003000 -76.283167                148                2460   \n",
       "4    0     1  36.906783 -76.089084                  0                2632   \n",
       "\n",
       "      VID  \n",
       "0  100008  \n",
       "1  100015  \n",
       "2  100016  \n",
       "3  100019  \n",
       "4  100016  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the `SEQUENCE DTTM` column into separate columns for 'hour' and a separate column to store the remaining time values in seconds \n",
    "ais_data[\"HOUR\"] = ais_data[\"SEQUENCE_DTTM\"].apply(lambda x: int(x.split(\":\")[0]))\n",
    "ais_data[\"TIME\"] = ais_data[\"SEQUENCE_DTTM\"].apply(lambda x: int(x.split(\":\")[0])*3600 + int(x.split(\":\")[1])*60 + int(x.split(\":\")[2]))\n",
    "# In the time column, we subtract the hour value from the time column to get the remaining time values in seconds\n",
    "ais_data[\"TIME\"] = ais_data[\"TIME\"] - ais_data[\"HOUR\"]*3600\n",
    "# Dropping the `SEQUENCE_DTTM` column\n",
    "ais_data.drop(\"SEQUENCE_DTTM\", axis=1, inplace=True)\n",
    "# Converting the hour column to give the hours starting from 0 instead of 14\n",
    "ais_data[\"HOUR\"] = ais_data[\"HOUR\"] - 14\n",
    "# converting the hour column to a categorical column\n",
    "ais_data[\"HOUR\"] = ais_data[\"HOUR\"].astype(\"category\")\n",
    "# Reordering the columns\n",
    "ais_data = ais_data[[\"HOUR\", \"TIME\", \"LAT\", \"LON\",  \"SPEED_OVER_GROUND\", \"COURSE_OVER_GROUND\", \"VID\"]]\n",
    "ais_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13714 entries, 0 to 13713\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype   \n",
      "---  ------              --------------  -----   \n",
      " 0   HOUR                13714 non-null  category\n",
      " 1   TIME                13714 non-null  int64   \n",
      " 2   LAT                 13714 non-null  float64 \n",
      " 3   LON                 13714 non-null  float64 \n",
      " 4   SPEED_OVER_GROUND   13714 non-null  int64   \n",
      " 5   COURSE_OVER_GROUND  13714 non-null  int64   \n",
      " 6   VID                 13714 non-null  int64   \n",
      "dtypes: category(1), float64(2), int64(4)\n",
      "memory usage: 656.6 KB\n"
     ]
    }
   ],
   "source": [
    "ais_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, VID is the label for each vessel. We will now split the data into features and labels for training.\n",
    "# In that regard, from the dataset, since the predictive value is in the columns (2-5), we will drop the following columns from the dataset as they have no predictive value: \n",
    "#   - HOUR\n",
    "#   - TIME\n",
    "# Then we will use the remaining columns as features. \n",
    "# The label will be the `VID` column\n",
    "def split_dataset_into_features_and_labels(dataset):\n",
    "    features = dataset.drop([\"HOUR\", \"TIME\", \"VID\"], axis=1)\n",
    "    labels = dataset[\"VID\"]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>SPEED_OVER_GROUND</th>\n",
       "      <th>COURSE_OVER_GROUND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.906850</td>\n",
       "      <td>-76.089022</td>\n",
       "      <td>1</td>\n",
       "      <td>1641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.950000</td>\n",
       "      <td>-76.026834</td>\n",
       "      <td>11</td>\n",
       "      <td>2815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.906783</td>\n",
       "      <td>-76.089084</td>\n",
       "      <td>0</td>\n",
       "      <td>2632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.003000</td>\n",
       "      <td>-76.283167</td>\n",
       "      <td>148</td>\n",
       "      <td>2460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.906783</td>\n",
       "      <td>-76.089084</td>\n",
       "      <td>0</td>\n",
       "      <td>2632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13709</th>\n",
       "      <td>36.916043</td>\n",
       "      <td>-76.168737</td>\n",
       "      <td>0</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13710</th>\n",
       "      <td>36.943300</td>\n",
       "      <td>-76.017700</td>\n",
       "      <td>248</td>\n",
       "      <td>2538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13711</th>\n",
       "      <td>36.906833</td>\n",
       "      <td>-76.089500</td>\n",
       "      <td>1</td>\n",
       "      <td>2495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13712</th>\n",
       "      <td>36.988868</td>\n",
       "      <td>-76.318949</td>\n",
       "      <td>82</td>\n",
       "      <td>2401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13713</th>\n",
       "      <td>36.943266</td>\n",
       "      <td>-76.017834</td>\n",
       "      <td>246</td>\n",
       "      <td>2523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13714 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             LAT        LON  SPEED_OVER_GROUND  COURSE_OVER_GROUND\n",
       "0      36.906850 -76.089022                  1                1641\n",
       "1      36.950000 -76.026834                 11                2815\n",
       "2      36.906783 -76.089084                  0                2632\n",
       "3      37.003000 -76.283167                148                2460\n",
       "4      36.906783 -76.089084                  0                2632\n",
       "...          ...        ...                ...                 ...\n",
       "13709  36.916043 -76.168737                  0                 363\n",
       "13710  36.943300 -76.017700                248                2538\n",
       "13711  36.906833 -76.089500                  1                2495\n",
       "13712  36.988868 -76.318949                 82                2401\n",
       "13713  36.943266 -76.017834                246                2523\n",
       "\n",
       "[13714 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels = split_dataset_into_features_and_labels(ais_data)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting:\n",
    "\n",
    "Since the dataset is a timeseries data, we will split it using the `SEQUENCE_DTTM` column. We will use the first 80% of the data for training, the next 15% for validation and the last 5% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set, Validation set and Test set \n",
    "X_train = features[:int(0.8*len(features))]\n",
    "y_train = labels[:int(0.8*len(labels))]\n",
    "X_val = features[int(0.8*len(features)):int(0.9*len(features))]\n",
    "y_val = labels[int(0.8*len(labels)):int(0.9*len(labels))]\n",
    "X_test = features[int(0.9*len(features)):]\n",
    "y_test = labels[int(0.9*len(labels)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10971, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.6906850e+01 -7.6089022e+01  1.0000000e+00  1.6410000e+03] 100008\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data: \n",
    "\n",
    "Here, we use the `MinMaxScaler` from `sklearn` to scale the data. We will use the `fit_transform` method to fit the scaler to the training data and then transform the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.9951913 ,  0.37725386, -0.99239544, -0.08808002]],\n",
       "\n",
       "       [[-0.39375566,  0.73277232, -0.91634981,  0.56432342]],\n",
       "\n",
       "       [[-0.99612517,  0.37689942, -1.        ,  0.46262851]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.24585685, -0.14046261, -0.77186312, -0.16976938]],\n",
       "\n",
       "       [[-0.99310056,  0.37452122, -1.        ,  0.73270353]],\n",
       "\n",
       "       [[ 0.52432922, -0.16812065, -0.26996198,  0.14754098]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing the features\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Converting the features into a 3D tensor\n",
    "X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
    "X_val_scaled = X_val_scaled.reshape(X_val_scaled.shape[0], 1, X_val_scaled.shape[1])\n",
    "X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
    "\n",
    "# Converting the labels into a 1D tensor\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "y_val = torch.from_numpy(y_val).type(torch.LongTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "# Printing the shapes of the features and labels\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.9951913   0.37725386 -0.99239544 -0.08808002]]\n",
      "\n",
      " [[-0.39375566  0.73277232 -0.91634981  0.56432342]]\n",
      "\n",
      " [[-0.99612517  0.37689942 -1.          0.46262851]]\n",
      "\n",
      " [[ 0.34497178 -0.73264083  0.12547529  0.3670464 ]]\n",
      "\n",
      " [[-0.99612517  0.37689942 -1.          0.46262851]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LSTM Model: \n",
    "An LSTM (Long Short_term Memory) is a special type of Recurrent Neural Network (RNN) that is capable of learning long-term dependencies through feedback connections. It can not only process single data points, but also entire sequences of data. It has feedback connections. It is through these “loops” in the network that it is able to store past information while still maintaining the ability to make predictions based on the current input. LSTMs have an additional memory cell and three gates. The memory cell is responsible for storing past information, and the three gates are responsible for controlling the flow of information into and out of the memory cell. The three gates are the input gate, the forget gate, and the output gate. The input gate controls the flow of information into the memory cell. The forget gate controls the flow of information out of the memory cell. The output gate controls the flow of information out of the memory cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, we will use a Long Short-Term Memory (LSTM) model to predict the vessel ID.\n",
    "# Here, the input size is 4, the hidden layer size is 100, and the output size is 2.\n",
    "# The number of layers is 2.\n",
    "# The input size is 4 because we have 4 features in the dataset.\n",
    "# These are the `LAT`, `LON`, `SPEED_OVER_GROUND`, and `COURSE_OVER_GROUND` columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size = 4, hidden_layer_size = 100, output_size = 20):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
    "                            torch.zeros(1,1,self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq)), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '1' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39mhidden_cell \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mzeros(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39mhidden_layer_size),\n\u001b[1;32m      6\u001b[0m                     torch\u001b[39m.\u001b[39mzeros(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39mhidden_layer_size))\n\u001b[0;32m----> 7\u001b[0m y_pred \u001b[39m=\u001b[39m model(seq)\n\u001b[1;32m      8\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m single_loss \u001b[39m=\u001b[39m loss_function(y_pred, labels)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1480\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1475\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1476\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1478\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1479\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1480\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1481\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [18], line 11\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input_seq)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_seq):\n\u001b[0;32m---> 11\u001b[0m     lstm_out, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_cell \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(input_seq\u001b[39m.\u001b[39;49mview(\u001b[39mlen\u001b[39;49m(input_seq)), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_cell)\n\u001b[1;32m     12\u001b[0m     predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(lstm_out\u001b[39m.\u001b[39mview(\u001b[39mlen\u001b[39m(input_seq), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m predictions[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '1' as a data type"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for i in range(epochs):\n",
    "    for seq, labels in zip(X_train_scaled, y_train):\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "        y_pred = model(seq)\n",
    "        labels = labels.view(1)\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "    # Printing the loss after every epoch\n",
    "    print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting values using the trained LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '1' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m     model\u001b[39m.\u001b[39mhidden \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mzeros(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39mhidden_layer_size),\n\u001b[1;32m     10\u001b[0m                     torch\u001b[39m.\u001b[39mzeros(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39mhidden_layer_size))\n\u001b[0;32m---> 11\u001b[0m     y_pred \u001b[39m=\u001b[39m model(seq)\n\u001b[1;32m     12\u001b[0m     predictions\u001b[39m.\u001b[39mappend(y_pred\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     13\u001b[0m     actual\u001b[39m.\u001b[39mappend(labels\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1480\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1475\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1476\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1478\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1479\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1480\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1481\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [18], line 11\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input_seq)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_seq):\n\u001b[0;32m---> 11\u001b[0m     lstm_out, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_cell \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(input_seq\u001b[39m.\u001b[39;49mview(\u001b[39mlen\u001b[39;49m(input_seq)), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_cell)\n\u001b[1;32m     12\u001b[0m     predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(lstm_out\u001b[39m.\u001b[39mview(\u001b[39mlen\u001b[39m(input_seq), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m predictions[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '1' as a data type"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "actual = []\n",
    "predictions = []\n",
    "\n",
    "if True:\n",
    "    for seq, labels in zip(X_test_scaled, y_test):\n",
    "        actual.append(labels.item())\n",
    "        with torch.no_grad():\n",
    "            model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                            torch.zeros(1, 1, model.hidden_layer_size))\n",
    "            y_pred = model(seq)\n",
    "            predictions.append(y_pred.item())\n",
    "            actual.append(labels.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the predicted and actual data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.from_numpy(np.array(predictions)).type(torch.Tensor)\n",
    "actual = torch.from_numpy(np.array(actual)).type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the normalized values back into their scalar forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final_predictions \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49minverse_transform(predictions)\n\u001b[1;32m      2\u001b[0m final_actual \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39minverse_transform(actual)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:525\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[39m\"\"\"Undo the scaling of X according to feature_range.\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \n\u001b[1;32m    513\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39m    Transformed data.\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    523\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 525\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    526\u001b[0m     X, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy, dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES, force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    527\u001b[0m )\n\u001b[1;32m    529\u001b[0m X \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_\n\u001b[1;32m    530\u001b[0m X \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.9/site-packages/sklearn/utils/validation.py:769\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    768\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 769\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    770\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    771\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    772\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    773\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    774\u001b[0m         )\n\u001b[1;32m    776\u001b[0m \u001b[39m# make sure we actually converted to numeric:\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mOUSV\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "final_predictions = scaler.inverse_transform(predictions)\n",
    "final_actual = scaler.inverse_transform(actual)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c78be42eb4a9b011f63865ede463952a3a1b8eab01bab5c6459dcc171b59c277"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
